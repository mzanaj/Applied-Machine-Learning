{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "name": "si670-hw1-mzanaj.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OWTe9VS3_b11"
      },
      "source": [
        "## SI 670 Applied Machine Learning, Week 1:  A simple classification task (Due 09/14 11:59pm)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYyEgEs9z4x5",
        "colab_type": "text"
      },
      "source": [
        "For this assignment, you will be using the Breast Cancer Wisconsin (Diagnostic) Database to create a classifier that can help diagnose patients. First, read through the description of the dataset (below). Then, try out the first one or two questions, which use basic numpy to prepare the data, so you can get familiar with the various columns, etc. Then use k-NN classifiers to learn and make predictions.\n",
        "\n",
        "Each question is worth 20 points, for a total of 100 points. Correct answers and code receive full credit, but partial credit will be awarded if you have the right idea even if your final answers aren't quite right.\n",
        "\n",
        "Submit your completed notebook file to the Canvas site - IMPORTANT: please name your submitted file si670-hw1-youruniqname.ipynb.\n",
        "\n",
        "As a reminder, the notebook code you submit must be your own work. Feel free to discuss general approaches to the homework with classmates: if you end up forming more of a team discussion on multiple questions, please include the names of the people you worked with at the top of your notebook file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAl_Xvj2z4x6",
        "colab_type": "text"
      },
      "source": [
        "### Put your name here: Martin Zanaj\n",
        "### Put your uniquename here: mzanaj"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "K7upHGCoupou",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc5d05aa-558b-4ede-9c82-bed1180e66b3"
      },
      "source": [
        "# import required modules and load data file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "#Load dataset & get some basic description\n",
        "cancer = load_breast_cancer()\n",
        "print(cancer.DESCR)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _breast_cancer_dataset:\n",
            "\n",
            "Breast cancer wisconsin (diagnostic) dataset\n",
            "--------------------------------------------\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    :Number of Instances: 569\n",
            "\n",
            "    :Number of Attributes: 30 numeric, predictive attributes and the class\n",
            "\n",
            "    :Attribute Information:\n",
            "        - radius (mean of distances from center to points on the perimeter)\n",
            "        - texture (standard deviation of gray-scale values)\n",
            "        - perimeter\n",
            "        - area\n",
            "        - smoothness (local variation in radius lengths)\n",
            "        - compactness (perimeter^2 / area - 1.0)\n",
            "        - concavity (severity of concave portions of the contour)\n",
            "        - concave points (number of concave portions of the contour)\n",
            "        - symmetry \n",
            "        - fractal dimension (\"coastline approximation\" - 1)\n",
            "\n",
            "        The mean, standard error, and \"worst\" or largest (mean of the three\n",
            "        largest values) of these features were computed for each image,\n",
            "        resulting in 30 features.  For instance, field 3 is Mean Radius, field\n",
            "        13 is Radius SE, field 23 is Worst Radius.\n",
            "\n",
            "        - class:\n",
            "                - WDBC-Malignant\n",
            "                - WDBC-Benign\n",
            "\n",
            "    :Summary Statistics:\n",
            "\n",
            "    ===================================== ====== ======\n",
            "                                           Min    Max\n",
            "    ===================================== ====== ======\n",
            "    radius (mean):                        6.981  28.11\n",
            "    texture (mean):                       9.71   39.28\n",
            "    perimeter (mean):                     43.79  188.5\n",
            "    area (mean):                          143.5  2501.0\n",
            "    smoothness (mean):                    0.053  0.163\n",
            "    compactness (mean):                   0.019  0.345\n",
            "    concavity (mean):                     0.0    0.427\n",
            "    concave points (mean):                0.0    0.201\n",
            "    symmetry (mean):                      0.106  0.304\n",
            "    fractal dimension (mean):             0.05   0.097\n",
            "    radius (standard error):              0.112  2.873\n",
            "    texture (standard error):             0.36   4.885\n",
            "    perimeter (standard error):           0.757  21.98\n",
            "    area (standard error):                6.802  542.2\n",
            "    smoothness (standard error):          0.002  0.031\n",
            "    compactness (standard error):         0.002  0.135\n",
            "    concavity (standard error):           0.0    0.396\n",
            "    concave points (standard error):      0.0    0.053\n",
            "    symmetry (standard error):            0.008  0.079\n",
            "    fractal dimension (standard error):   0.001  0.03\n",
            "    radius (worst):                       7.93   36.04\n",
            "    texture (worst):                      12.02  49.54\n",
            "    perimeter (worst):                    50.41  251.2\n",
            "    area (worst):                         185.2  4254.0\n",
            "    smoothness (worst):                   0.071  0.223\n",
            "    compactness (worst):                  0.027  1.058\n",
            "    concavity (worst):                    0.0    1.252\n",
            "    concave points (worst):               0.0    0.291\n",
            "    symmetry (worst):                     0.156  0.664\n",
            "    fractal dimension (worst):            0.055  0.208\n",
            "    ===================================== ====== ======\n",
            "\n",
            "    :Missing Attribute Values: None\n",
            "\n",
            "    :Class Distribution: 212 - Malignant, 357 - Benign\n",
            "\n",
            "    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\n",
            "\n",
            "    :Donor: Nick Street\n",
            "\n",
            "    :Date: November, 1995\n",
            "\n",
            "This is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\n",
            "https://goo.gl/U2Uwz2\n",
            "\n",
            "Features are computed from a digitized image of a fine needle\n",
            "aspirate (FNA) of a breast mass.  They describe\n",
            "characteristics of the cell nuclei present in the image.\n",
            "\n",
            "Separating plane described above was obtained using\n",
            "Multisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\n",
            "Construction Via Linear Programming.\" Proceedings of the 4th\n",
            "Midwest Artificial Intelligence and Cognitive Science Society,\n",
            "pp. 97-101, 1992], a classification method which uses linear\n",
            "programming to construct a decision tree.  Relevant features\n",
            "were selected using an exhaustive search in the space of 1-4\n",
            "features and 1-3 separating planes.\n",
            "\n",
            "The actual linear program used to obtain the separating plane\n",
            "in the 3-dimensional space is that described in:\n",
            "[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\n",
            "Programming Discrimination of Two Linearly Inseparable Sets\",\n",
            "Optimization Methods and Software 1, 1992, 23-34].\n",
            "\n",
            "This database is also available through the UW CS ftp server:\n",
            "\n",
            "ftp ftp.cs.wisc.edu\n",
            "cd math-prog/cpo-dataset/machine-learn/WDBC/\n",
            "\n",
            ".. topic:: References\n",
            "\n",
            "   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \n",
            "     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \n",
            "     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\n",
            "     San Jose, CA, 1993.\n",
            "   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \n",
            "     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \n",
            "     July-August 1995.\n",
            "   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\n",
            "     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \n",
            "     163-171.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "MEGrUkGz_b11"
      },
      "source": [
        "The object returned by `load_breast_cancer()` is a scikit-learn Bunch object, which is similar to a dictionary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_Zd27xZ1_b12",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "97b3a71b-33cf-459c-81e6-8eb79cc834a7"
      },
      "source": [
        "#The bunch object\n",
        "cancer.keys()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['data', 'target', 'target_names', 'DESCR', 'feature_names', 'filename'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fuCKe7Hhz4yF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        },
        "outputId": "dc396ec7-aff7-47a8-d4ed-cee512f60af1"
      },
      "source": [
        "#Column names (target is missing given this reppresents just the X matrix and not or Y)\n",
        "cancer.feature_names"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
              "       'mean smoothness', 'mean compactness', 'mean concavity',\n",
              "       'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
              "       'radius error', 'texture error', 'perimeter error', 'area error',\n",
              "       'smoothness error', 'compactness error', 'concavity error',\n",
              "       'concave points error', 'symmetry error',\n",
              "       'fractal dimension error', 'worst radius', 'worst texture',\n",
              "       'worst perimeter', 'worst area', 'worst smoothness',\n",
              "       'worst compactness', 'worst concavity', 'worst concave points',\n",
              "       'worst symmetry', 'worst fractal dimension'], dtype='<U23')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34uG5t2Az4yM",
        "colab_type": "text"
      },
      "source": [
        "### Question 1\n",
        "\n",
        "Scikit-learn works with lists, numpy arrays, scipy-sparse matrices, and pandas DataFrames, so converting the dataset to a DataFrame is not necessary for training this model. Using a DataFrame does however help make many things easier such as manipulating data, so let's practice creating a classifier with a pandas DataFrame. \n",
        "\n",
        "\n",
        "\n",
        "Convert the sklearn.dataset `cancer` to a DataFrame. \n",
        "\n",
        "* This function should return a `(569, 31)` DataFrame with columns =  \n",
        "\n",
        "\n",
        "\n",
        "    ['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
        "    'mean smoothness', 'mean compactness', 'mean concavity',\n",
        "    'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
        "    'radius error', 'texture error', 'perimeter error', 'area error',\n",
        "    'smoothness error', 'compactness error', 'concavity error',\n",
        "    'concave points error', 'symmetry error', 'fractal dimension error',\n",
        "    'worst radius', 'worst texture', 'worst perimeter', 'worst area',\n",
        "    'worst smoothness', 'worst compactness', 'worst concavity',\n",
        "    'worst concave points', 'worst symmetry', 'worst fractal dimension',\n",
        "    'target']\n",
        "\n",
        "and index = \n",
        "\n",
        "    RangeIndex(start=0, stop=569, step=1)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ANQFzXaz4yN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 479
        },
        "outputId": "8e666af5-4cd0-41d5-c146-9b090e6ec310"
      },
      "source": [
        "def answer_one():\n",
        "    #full dataset = predictors + target \n",
        "    #Create dataset, by first getting the X portion, and then addind target to it. Finally update the column names\n",
        "    d=pd.DataFrame(cancer.data) \n",
        "    d['target']= cancer.target\n",
        "    col=cancer.feature_names.tolist()\n",
        "    col.append('target')\n",
        "    d.columns = col\n",
        "    d.index = pd.RangeIndex(start=0, stop=569, step=1)\n",
        "    \n",
        "    return pd.DataFrame(d)\n",
        "\n",
        "answer_one()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mean radius</th>\n",
              "      <th>mean texture</th>\n",
              "      <th>mean perimeter</th>\n",
              "      <th>mean area</th>\n",
              "      <th>mean smoothness</th>\n",
              "      <th>mean compactness</th>\n",
              "      <th>mean concavity</th>\n",
              "      <th>mean concave points</th>\n",
              "      <th>mean symmetry</th>\n",
              "      <th>mean fractal dimension</th>\n",
              "      <th>radius error</th>\n",
              "      <th>texture error</th>\n",
              "      <th>perimeter error</th>\n",
              "      <th>area error</th>\n",
              "      <th>smoothness error</th>\n",
              "      <th>compactness error</th>\n",
              "      <th>concavity error</th>\n",
              "      <th>concave points error</th>\n",
              "      <th>symmetry error</th>\n",
              "      <th>fractal dimension error</th>\n",
              "      <th>worst radius</th>\n",
              "      <th>worst texture</th>\n",
              "      <th>worst perimeter</th>\n",
              "      <th>worst area</th>\n",
              "      <th>worst smoothness</th>\n",
              "      <th>worst compactness</th>\n",
              "      <th>worst concavity</th>\n",
              "      <th>worst concave points</th>\n",
              "      <th>worst symmetry</th>\n",
              "      <th>worst fractal dimension</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>17.99</td>\n",
              "      <td>10.38</td>\n",
              "      <td>122.80</td>\n",
              "      <td>1001.0</td>\n",
              "      <td>0.11840</td>\n",
              "      <td>0.27760</td>\n",
              "      <td>0.30010</td>\n",
              "      <td>0.14710</td>\n",
              "      <td>0.2419</td>\n",
              "      <td>0.07871</td>\n",
              "      <td>1.0950</td>\n",
              "      <td>0.9053</td>\n",
              "      <td>8.589</td>\n",
              "      <td>153.40</td>\n",
              "      <td>0.006399</td>\n",
              "      <td>0.04904</td>\n",
              "      <td>0.05373</td>\n",
              "      <td>0.01587</td>\n",
              "      <td>0.03003</td>\n",
              "      <td>0.006193</td>\n",
              "      <td>25.380</td>\n",
              "      <td>17.33</td>\n",
              "      <td>184.60</td>\n",
              "      <td>2019.0</td>\n",
              "      <td>0.16220</td>\n",
              "      <td>0.66560</td>\n",
              "      <td>0.7119</td>\n",
              "      <td>0.2654</td>\n",
              "      <td>0.4601</td>\n",
              "      <td>0.11890</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20.57</td>\n",
              "      <td>17.77</td>\n",
              "      <td>132.90</td>\n",
              "      <td>1326.0</td>\n",
              "      <td>0.08474</td>\n",
              "      <td>0.07864</td>\n",
              "      <td>0.08690</td>\n",
              "      <td>0.07017</td>\n",
              "      <td>0.1812</td>\n",
              "      <td>0.05667</td>\n",
              "      <td>0.5435</td>\n",
              "      <td>0.7339</td>\n",
              "      <td>3.398</td>\n",
              "      <td>74.08</td>\n",
              "      <td>0.005225</td>\n",
              "      <td>0.01308</td>\n",
              "      <td>0.01860</td>\n",
              "      <td>0.01340</td>\n",
              "      <td>0.01389</td>\n",
              "      <td>0.003532</td>\n",
              "      <td>24.990</td>\n",
              "      <td>23.41</td>\n",
              "      <td>158.80</td>\n",
              "      <td>1956.0</td>\n",
              "      <td>0.12380</td>\n",
              "      <td>0.18660</td>\n",
              "      <td>0.2416</td>\n",
              "      <td>0.1860</td>\n",
              "      <td>0.2750</td>\n",
              "      <td>0.08902</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>19.69</td>\n",
              "      <td>21.25</td>\n",
              "      <td>130.00</td>\n",
              "      <td>1203.0</td>\n",
              "      <td>0.10960</td>\n",
              "      <td>0.15990</td>\n",
              "      <td>0.19740</td>\n",
              "      <td>0.12790</td>\n",
              "      <td>0.2069</td>\n",
              "      <td>0.05999</td>\n",
              "      <td>0.7456</td>\n",
              "      <td>0.7869</td>\n",
              "      <td>4.585</td>\n",
              "      <td>94.03</td>\n",
              "      <td>0.006150</td>\n",
              "      <td>0.04006</td>\n",
              "      <td>0.03832</td>\n",
              "      <td>0.02058</td>\n",
              "      <td>0.02250</td>\n",
              "      <td>0.004571</td>\n",
              "      <td>23.570</td>\n",
              "      <td>25.53</td>\n",
              "      <td>152.50</td>\n",
              "      <td>1709.0</td>\n",
              "      <td>0.14440</td>\n",
              "      <td>0.42450</td>\n",
              "      <td>0.4504</td>\n",
              "      <td>0.2430</td>\n",
              "      <td>0.3613</td>\n",
              "      <td>0.08758</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11.42</td>\n",
              "      <td>20.38</td>\n",
              "      <td>77.58</td>\n",
              "      <td>386.1</td>\n",
              "      <td>0.14250</td>\n",
              "      <td>0.28390</td>\n",
              "      <td>0.24140</td>\n",
              "      <td>0.10520</td>\n",
              "      <td>0.2597</td>\n",
              "      <td>0.09744</td>\n",
              "      <td>0.4956</td>\n",
              "      <td>1.1560</td>\n",
              "      <td>3.445</td>\n",
              "      <td>27.23</td>\n",
              "      <td>0.009110</td>\n",
              "      <td>0.07458</td>\n",
              "      <td>0.05661</td>\n",
              "      <td>0.01867</td>\n",
              "      <td>0.05963</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>14.910</td>\n",
              "      <td>26.50</td>\n",
              "      <td>98.87</td>\n",
              "      <td>567.7</td>\n",
              "      <td>0.20980</td>\n",
              "      <td>0.86630</td>\n",
              "      <td>0.6869</td>\n",
              "      <td>0.2575</td>\n",
              "      <td>0.6638</td>\n",
              "      <td>0.17300</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>20.29</td>\n",
              "      <td>14.34</td>\n",
              "      <td>135.10</td>\n",
              "      <td>1297.0</td>\n",
              "      <td>0.10030</td>\n",
              "      <td>0.13280</td>\n",
              "      <td>0.19800</td>\n",
              "      <td>0.10430</td>\n",
              "      <td>0.1809</td>\n",
              "      <td>0.05883</td>\n",
              "      <td>0.7572</td>\n",
              "      <td>0.7813</td>\n",
              "      <td>5.438</td>\n",
              "      <td>94.44</td>\n",
              "      <td>0.011490</td>\n",
              "      <td>0.02461</td>\n",
              "      <td>0.05688</td>\n",
              "      <td>0.01885</td>\n",
              "      <td>0.01756</td>\n",
              "      <td>0.005115</td>\n",
              "      <td>22.540</td>\n",
              "      <td>16.67</td>\n",
              "      <td>152.20</td>\n",
              "      <td>1575.0</td>\n",
              "      <td>0.13740</td>\n",
              "      <td>0.20500</td>\n",
              "      <td>0.4000</td>\n",
              "      <td>0.1625</td>\n",
              "      <td>0.2364</td>\n",
              "      <td>0.07678</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>564</th>\n",
              "      <td>21.56</td>\n",
              "      <td>22.39</td>\n",
              "      <td>142.00</td>\n",
              "      <td>1479.0</td>\n",
              "      <td>0.11100</td>\n",
              "      <td>0.11590</td>\n",
              "      <td>0.24390</td>\n",
              "      <td>0.13890</td>\n",
              "      <td>0.1726</td>\n",
              "      <td>0.05623</td>\n",
              "      <td>1.1760</td>\n",
              "      <td>1.2560</td>\n",
              "      <td>7.673</td>\n",
              "      <td>158.70</td>\n",
              "      <td>0.010300</td>\n",
              "      <td>0.02891</td>\n",
              "      <td>0.05198</td>\n",
              "      <td>0.02454</td>\n",
              "      <td>0.01114</td>\n",
              "      <td>0.004239</td>\n",
              "      <td>25.450</td>\n",
              "      <td>26.40</td>\n",
              "      <td>166.10</td>\n",
              "      <td>2027.0</td>\n",
              "      <td>0.14100</td>\n",
              "      <td>0.21130</td>\n",
              "      <td>0.4107</td>\n",
              "      <td>0.2216</td>\n",
              "      <td>0.2060</td>\n",
              "      <td>0.07115</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>565</th>\n",
              "      <td>20.13</td>\n",
              "      <td>28.25</td>\n",
              "      <td>131.20</td>\n",
              "      <td>1261.0</td>\n",
              "      <td>0.09780</td>\n",
              "      <td>0.10340</td>\n",
              "      <td>0.14400</td>\n",
              "      <td>0.09791</td>\n",
              "      <td>0.1752</td>\n",
              "      <td>0.05533</td>\n",
              "      <td>0.7655</td>\n",
              "      <td>2.4630</td>\n",
              "      <td>5.203</td>\n",
              "      <td>99.04</td>\n",
              "      <td>0.005769</td>\n",
              "      <td>0.02423</td>\n",
              "      <td>0.03950</td>\n",
              "      <td>0.01678</td>\n",
              "      <td>0.01898</td>\n",
              "      <td>0.002498</td>\n",
              "      <td>23.690</td>\n",
              "      <td>38.25</td>\n",
              "      <td>155.00</td>\n",
              "      <td>1731.0</td>\n",
              "      <td>0.11660</td>\n",
              "      <td>0.19220</td>\n",
              "      <td>0.3215</td>\n",
              "      <td>0.1628</td>\n",
              "      <td>0.2572</td>\n",
              "      <td>0.06637</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>566</th>\n",
              "      <td>16.60</td>\n",
              "      <td>28.08</td>\n",
              "      <td>108.30</td>\n",
              "      <td>858.1</td>\n",
              "      <td>0.08455</td>\n",
              "      <td>0.10230</td>\n",
              "      <td>0.09251</td>\n",
              "      <td>0.05302</td>\n",
              "      <td>0.1590</td>\n",
              "      <td>0.05648</td>\n",
              "      <td>0.4564</td>\n",
              "      <td>1.0750</td>\n",
              "      <td>3.425</td>\n",
              "      <td>48.55</td>\n",
              "      <td>0.005903</td>\n",
              "      <td>0.03731</td>\n",
              "      <td>0.04730</td>\n",
              "      <td>0.01557</td>\n",
              "      <td>0.01318</td>\n",
              "      <td>0.003892</td>\n",
              "      <td>18.980</td>\n",
              "      <td>34.12</td>\n",
              "      <td>126.70</td>\n",
              "      <td>1124.0</td>\n",
              "      <td>0.11390</td>\n",
              "      <td>0.30940</td>\n",
              "      <td>0.3403</td>\n",
              "      <td>0.1418</td>\n",
              "      <td>0.2218</td>\n",
              "      <td>0.07820</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>567</th>\n",
              "      <td>20.60</td>\n",
              "      <td>29.33</td>\n",
              "      <td>140.10</td>\n",
              "      <td>1265.0</td>\n",
              "      <td>0.11780</td>\n",
              "      <td>0.27700</td>\n",
              "      <td>0.35140</td>\n",
              "      <td>0.15200</td>\n",
              "      <td>0.2397</td>\n",
              "      <td>0.07016</td>\n",
              "      <td>0.7260</td>\n",
              "      <td>1.5950</td>\n",
              "      <td>5.772</td>\n",
              "      <td>86.22</td>\n",
              "      <td>0.006522</td>\n",
              "      <td>0.06158</td>\n",
              "      <td>0.07117</td>\n",
              "      <td>0.01664</td>\n",
              "      <td>0.02324</td>\n",
              "      <td>0.006185</td>\n",
              "      <td>25.740</td>\n",
              "      <td>39.42</td>\n",
              "      <td>184.60</td>\n",
              "      <td>1821.0</td>\n",
              "      <td>0.16500</td>\n",
              "      <td>0.86810</td>\n",
              "      <td>0.9387</td>\n",
              "      <td>0.2650</td>\n",
              "      <td>0.4087</td>\n",
              "      <td>0.12400</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>568</th>\n",
              "      <td>7.76</td>\n",
              "      <td>24.54</td>\n",
              "      <td>47.92</td>\n",
              "      <td>181.0</td>\n",
              "      <td>0.05263</td>\n",
              "      <td>0.04362</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.1587</td>\n",
              "      <td>0.05884</td>\n",
              "      <td>0.3857</td>\n",
              "      <td>1.4280</td>\n",
              "      <td>2.548</td>\n",
              "      <td>19.15</td>\n",
              "      <td>0.007189</td>\n",
              "      <td>0.00466</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.02676</td>\n",
              "      <td>0.002783</td>\n",
              "      <td>9.456</td>\n",
              "      <td>30.37</td>\n",
              "      <td>59.16</td>\n",
              "      <td>268.6</td>\n",
              "      <td>0.08996</td>\n",
              "      <td>0.06444</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.2871</td>\n",
              "      <td>0.07039</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>569 rows × 31 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     mean radius  mean texture  ...  worst fractal dimension  target\n",
              "0          17.99         10.38  ...                  0.11890       0\n",
              "1          20.57         17.77  ...                  0.08902       0\n",
              "2          19.69         21.25  ...                  0.08758       0\n",
              "3          11.42         20.38  ...                  0.17300       0\n",
              "4          20.29         14.34  ...                  0.07678       0\n",
              "..           ...           ...  ...                      ...     ...\n",
              "564        21.56         22.39  ...                  0.07115       0\n",
              "565        20.13         28.25  ...                  0.06637       0\n",
              "566        16.60         28.08  ...                  0.07820       0\n",
              "567        20.60         29.33  ...                  0.12400       0\n",
              "568         7.76         24.54  ...                  0.07039       1\n",
              "\n",
              "[569 rows x 31 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZWecnncz4yQ",
        "colab_type": "text"
      },
      "source": [
        "### Question 2\n",
        "Using `train_test_split`, split the dataset into training and test sets `(X_train, X_test, y_train, and y_test)`.\n",
        "\n",
        "**Set the random number generator state to 670 using `random_state=670` to make sure your results match ours **\n",
        "\n",
        "*This function should return a tuple of length 4:* `(X_train, X_test, y_train, y_test)`*, where* \n",
        "* `X_train` *has shape* `(450, 30)`\n",
        "* `X_test` *has shape* `(119, 30)`\n",
        "* `y_train` *has shape* `(450,)`\n",
        "* `y_test` *has shape* `(119,)`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VVmIlPGUz4yR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 972
        },
        "outputId": "de78fd61-1ad3-4451-ad6e-c2c6815c8091"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def answer_two():\n",
        "    df = answer_one()\n",
        "    \n",
        "    #Get X & y (predictor space & response)\n",
        "    X=df.drop(['target'],axis=1)\n",
        "    y= df['target']\n",
        "    \n",
        "    #Split data into training & testing \n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=670,train_size=0.792)\n",
        "\n",
        "    #Testing purposes, size of each component depends on train_size parameter\n",
        "    #   print(X_train.shape)\n",
        "    #   print(X_test.shape)\n",
        "    #   print(y_train.shape)\n",
        "    #   print(y_test.shape)\n",
        "    \n",
        "    return X_train, X_test, y_train, y_test\n",
        "\n",
        "answer_two()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
              " 519       12.750         16.70  ...          0.3071                  0.08557\n",
              " 289       11.370         18.89  ...          0.3267                  0.06994\n",
              " 406       16.140         14.86  ...          0.2778                  0.07012\n",
              " 272       21.750         20.99  ...          0.2833                  0.08858\n",
              " 525        8.571         13.10  ...          0.2983                  0.10490\n",
              " ..           ...           ...  ...             ...                      ...\n",
              " 151        8.219         20.70  ...          0.3322                  0.14860\n",
              " 528       13.940         13.17  ...          0.2160                  0.07253\n",
              " 364       13.400         16.95  ...          0.2741                  0.07582\n",
              " 409       12.270         17.92  ...          0.3455                  0.06896\n",
              " 562       15.220         30.62  ...          0.4089                  0.14090\n",
              " \n",
              " [450 rows x 30 columns],\n",
              "      mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
              " 158        12.06         12.74  ...          0.2514                  0.07898\n",
              " 348        11.47         16.03  ...          0.2851                  0.08763\n",
              " 212        28.11         18.47  ...          0.1648                  0.05525\n",
              " 347        14.76         14.74  ...          0.3109                  0.08187\n",
              " 67         11.31         19.04  ...          0.2400                  0.06641\n",
              " ..           ...           ...  ...             ...                      ...\n",
              " 400        17.91         21.02  ...          0.3245                  0.11980\n",
              " 229        12.83         22.33  ...          0.3407                  0.12430\n",
              " 243        13.75         23.77  ...          0.2663                  0.06321\n",
              " 240        13.64         15.60  ...          0.2530                  0.06510\n",
              " 91         15.37         22.76  ...          0.2556                  0.06828\n",
              " \n",
              " [119 rows x 30 columns],\n",
              " 519    1\n",
              " 289    1\n",
              " 406    1\n",
              " 272    0\n",
              " 525    1\n",
              "       ..\n",
              " 151    1\n",
              " 528    1\n",
              " 364    1\n",
              " 409    1\n",
              " 562    0\n",
              " Name: target, Length: 450, dtype: int64,\n",
              " 158    1\n",
              " 348    1\n",
              " 212    0\n",
              " 347    1\n",
              " 67     1\n",
              "       ..\n",
              " 400    0\n",
              " 229    0\n",
              " 243    1\n",
              " 240    1\n",
              " 91     0\n",
              " Name: target, Length: 119, dtype: int64)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sVnk0iYDz4yV",
        "colab_type": "text"
      },
      "source": [
        "### Question 3\n",
        "Using KNeighborsClassifier, fit a k-nearest neighbors (knn) classifier with `n_neighbors = 5` on `X_train`, `y_train`. Then evaluate the classifier accuracy using `score` function on `X_test` and `y_test`.\n",
        "\n",
        "*This function should return a tuple of (knn, accuracy), where*\n",
        "* `knn` is a `sklearn.neighbors.classification.KNeighborsClassifier`\n",
        "* `accuracy` is a `float` number returned by the `score` function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F3mJjcGuz4yV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "26fe52bb-e3b5-492b-8a8e-0a74eaff8bd8"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "\n",
        "def answer_three():\n",
        "    #Training & testing \n",
        "    X_train, X_test, y_train, y_test = answer_two()\n",
        "\n",
        "    #KNN with k=5\n",
        "    knn= KNeighborsClassifier(n_neighbors= 5)\n",
        "\n",
        "    #fit\n",
        "    knn.fit(X_train,y_train)\n",
        "    \n",
        "    #accuracy (total right/total observations)\n",
        "    accuracy = knn.score(X_test, y_test)\n",
        "\n",
        "    return (knn, accuracy)\n",
        "\n",
        "answer_three()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                      weights='uniform'), 0.9327731092436975)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmrRgu-Rz4yd",
        "colab_type": "text"
      },
      "source": [
        "### Question 4\n",
        "Recall in the fruits example in lab1, we found the feature scales matter. In this question, please examine the mean and standard deviation of `X_train`, and use the `sklearn.preprocessing.StandardScaler` to normalize the feature. Then train another knn (k=5) classifier and evaluate it.\n",
        "\n",
        "*This function should return a tuple of (standardized_X_train, knn, accuracy), where*\n",
        "* `standardized_X_train` is a `pandas.DataFrame` of the standardized features\n",
        "* `knn` is a `sklearn.neighbors.classification.KNeighborsClassifier`\n",
        "* `accuracy` is a `float` number returned by the `score` function\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A4l8kH-az4ye",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348
        },
        "outputId": "cd0d0143-bb06-44ae-b409-72cef2bb0999"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "def answer_four(k=5):\n",
        "    X_train, X_test, y_train, y_test = answer_two()\n",
        "  \n",
        "    #We have some values that have quite large SD, this can lead to potential bad results (highest values will dominate over smaller ones influencing final fit ) \n",
        "    sd= np.std(X_train)\n",
        "    sd=sd[sd>10]\n",
        "    #print(sd1)\n",
        "    ''' mean perimeter      24.096333\n",
        "        mean area          345.605364\n",
        "        area error          41.698021\n",
        "        worst perimeter     33.600851\n",
        "        worst area         575.154195 '''\n",
        "\n",
        "    #Potential solution is to standardize/normalize the data (mean=0, sd=1)\n",
        "    scaler = StandardScaler()\n",
        "    columns = X_train.columns\n",
        "    standardized_X_train = X_train.copy()\n",
        "    standardized_X_test = X_test.copy()\n",
        "\n",
        "    #Standardize both training & testing datasets\n",
        "    standardized_X_train[columns] = scaler.fit_transform(X_train[columns])\n",
        "    standardized_X_test[columns] = scaler.transform(X_test[columns])\n",
        "\n",
        "    #Fit knn to standardized observations\n",
        "    knn=KNeighborsClassifier(n_neighbors=5)\n",
        "    knn.fit(standardized_X_train, y_train)\n",
        "\n",
        "    #The accuracy should indeed improve \n",
        "    accuracy = knn.score(standardized_X_test, y_test)\n",
        "\n",
        "    return (standardized_X_train, knn, accuracy)  \n",
        "\n",
        "answer_four()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(     mean radius  mean texture  ...  worst symmetry  worst fractal dimension\n",
              " 519    -0.350908     -0.594288  ...        0.243782                 0.054929\n",
              " 289    -0.745705     -0.092213  ...        0.556914                -0.789683\n",
              " 406     0.618920     -1.016123  ...       -0.224319                -0.779956\n",
              " 272     2.223855      0.389229  ...       -0.136450                 0.217583\n",
              " 525    -1.546456     -1.419617  ...        0.103192                 1.099480\n",
              " ..           ...           ...  ...             ...                      ...\n",
              " 151    -1.647158      0.322745  ...        0.644783                 3.460934\n",
              " 528    -0.010467     -1.403569  ...       -1.211645                -0.649725\n",
              " 364    -0.164953     -0.536973  ...       -0.283431                -0.471940\n",
              " 409    -0.488228     -0.314593  ...        0.857266                -0.842640\n",
              " 562     0.355722      2.596985  ...        1.870153                 3.044842\n",
              " \n",
              " [450 rows x 30 columns],\n",
              " KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
              "                      metric_params=None, n_jobs=None, n_neighbors=5, p=2,\n",
              "                      weights='uniform'),\n",
              " 0.957983193277311)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hc7-SUf0z4yh",
        "colab_type": "text"
      },
      "source": [
        "### Question 5\n",
        "Recall in the lecture and lab 1, we found the choice of k could affect the prediction accuracy. In this question, please train knn classifiers with `k = 1,2,...,20` and evaluate the classifiers by features normalized with `sklearn.preprocessing.StandardScaler` (similar to Question 4 with k changed). Next, please visualize how sensitive the k-NN classification accuracy is to the choice of the 'k' parameter by a scatter plot with the `x-axis` representing `k` and the `y-axis` representing `accuracy`.\n",
        "\n",
        "* This function should get the accuracy under each k and store them in `accracy_list` for the scatter plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wSWjkKWuz4yh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "23f5a146-d5c7-49a0-b70b-09c0cb56ec36"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def answer_five():\n",
        "    k_range = range(1,21)\n",
        "    accuracy_list = []\n",
        "    \n",
        "    #Run a knn fit for each k [1 through 20]\n",
        "    for k in k_range:\n",
        "      #Training & testing dataset\n",
        "      X_train, X_test, y_train, y_test = answer_two()\n",
        "\n",
        "      #Standardize the predictor space\n",
        "      scaler = StandardScaler()\n",
        "      columns = X_train.columns\n",
        "      standardized_X_train = X_train.copy()\n",
        "      standardized_X_test =  X_test.copy()\n",
        "\n",
        "      standardized_X_train[columns] = scaler.fit_transform(X_train[columns])\n",
        "      standardized_X_test[columns] = scaler.transform(X_test[columns])\n",
        "\n",
        "      #Runn knn fit with specific k [1:20]\n",
        "      knn=KNeighborsClassifier(n_neighbors=k)\n",
        "      knn.fit(standardized_X_train, y_train)\n",
        "\n",
        "      #Get accuracy \n",
        "      accuracy = knn.score(standardized_X_test, y_test)\n",
        "      accuracy_list.append(accuracy)\n",
        "\n",
        "    #Plot accuracy against choice of k \n",
        "    plt.figure()\n",
        "    plt.title('Accuracy as a function of X',fontweight=\"bold\")\n",
        "    plt.xlabel('k')\n",
        "    plt.ylabel('accuracy')\n",
        "    plt.scatter(k_range, accuracy_list)\n",
        "    plt.xticks([0,5,10,15,20])\n",
        "\n",
        "answer_five()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAeqUlEQVR4nO3de7wcZZ3n8c/XXCAKGiQZliSQgMbomcVNxiPja7xw2ddsgjeSyCh4GXRccVeYhVUykHFWmcyw6IYRb4wjSriMXGQghMyMGpiQeJlZXE5MIJB4NBtRcoIQXY5yOUASfvtHPR0qnT4nXelTqdOnv+/Xq1/d9VQ9Vb96Tp/+dT1PdZUiAjMzs2a9qOoAzMysvThxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmI5CkEyT1SHpOUkj69xXGEukxo6oYUhyfkbQjxXJrlbF0OicOa0iZh3IfGq+pOqYO81ngdUAP8AXgV2VvUNIl6W99bd2sL6THb8uOYTCSTgQuAg4DvgL8U4NlvpTi/1dJL0plUyX1p/LTD27Uo9fYqgOwEevNwPTc9AeAP68iEEnjImJnFduu0KvS819ExN1VBhIRF1S5/aTWHvdGxMcGWeZi4G3AHwDnA1cAfwe8DLgpIu4oPcpOERF++LHPA7gKCOBH6fkhQLn5Lwb+EvgxMABsAz6S5o0l+8d9AHgaeBT4VJp3bVrfJWl6RpqO3LojPS4AfgZsTeU3An3As8ATwN3ACbl6Lwe+CPxf4BlgK/B24L1pfXfmln1PfVnd/l8I/BR4Km3vPuCM3Pw/BNal+b9J7bRwkHW9FrgHeBzYCTwCfBkYP8jyD+XaYE/b5KZnpOlL0vS1afqDafoHZB+a/am93re/v1tuXfnH2kG2Oxn4OvALsqOQe4B5uW3U/sZ/B/xjeg/cD8we4v026Dpz+5V/XDLIek4Bnk/b/HRa9lFgUtX/U6Pp4a4q24ekQ4Az0uQnyD7wpgNvyS32NeBTwO8AN5F9cNa+Ff4l8HngeOA24LvAqw8glP8JfA+4M01PB9aSfcD8iOxD4pYU84uAFcCfAocA3yBLHMcDy9M+nCrp6LSuWrfFjYNs+zhgI9mH4B3A7wLfyPXzXwP8h7R/t5F9WA02DjEZeC4ttwzYDZwLfHyQ5ZeRJUZSnS8Mstxg3pge/weYAnxV0kvTvMH+bvcAP0zLbE7b3GccIbXzSuDDZN1nd5B1qf2zpD+oW/yjwC6y5H8C8KVGwTaxzk3AXWnxvhTbPY3WFRFryBLWBLJkCHBuRJTe1ddRqs5cfoy8B7CQF76pvQi4Pk1/Lc2fxAvf/Obk6o0DRPahF8CC/Lz0fC3NH3H8SV1cU8kSw2fIPoRqy00ButPrAeDoBtutLf9xsiOix9OyLx2kDV4C/DHwV2Tf3h9J9d+b5j8KPEmWYGeldhozRJu+BVgMfA5YzRBHO2n5h9IyJzdolxlp+hIaH3H8Gjg0/T12pbLuof5ujdbXaLvAien1E8BL0vwrUtmNdX/jf07Tp6TpJwfZ12bWWdu3tU28f2flYt421N/FjwN7eIzDGnl/ev7HiHhe0u1kYxx/JOk8sm/jAM9GxPpapYjYKWky2QAm5L4VxuBjFGOGiONfay8kzST7dnxYg+Um52L6RUQ80mC7XwfOS/u2AZgI3BoR+wz4ShqfYm90BDE5PX8UWAr8Q5r+dVr/zQ3Wt5js6GmwdR2owdpuc0Q8k7b9FPBSsnYb9O9WYJsz0vPDEfFUev3j9Dy9btnaNvrT80uGYZ3N+Fx6DrIvG/8duPwA1mODcFeV7UXSROCtafLDkoKsqweyQcZ3kHU9ABwiaXau7liyroYnU9Hv182DbEwAsg8zGLx7B7KxhZq3kX341T70j8qHnYvpWEn/rn67EXEf2ZjEHLJBVBi8m6orxbULeAXZ/8mm3LYAvh0RM8m+xZ8BHAlcOsj63pOe/4LsaOeiunU16+n0vL+225V7nb/89VB/N8i60GDoz4WH0vMxkl6cXs9Kzz8fJI5gaEXWOSRJf0z2/n0MeHfa9hJJrxqyohXiIw6r926yMYLfAmty5V3ATOADEXGrpBvJBp1XS1oBHAFsiYg/k/RFsjOwbpB0G1mXyW6yb/u1b6FnS9oFvK/JuB5Nz68i6+OeXTf/R8D3yc4Gu1fSd4BpwLfJBswhO+p4HdnAdj/wrUG29SuyMYuxwN8Ah6d9z1sv6SGywdxjUlk/jdVifx/wSmD+IMvtz3qysYsvS+rlhXGapkTErwb7uwF/BjycFj1N0pfIuoVuq1tND9lYyO8D35f0IHAW2Qf03x7Ybg3POtMXhs+nyXPT+/QrwMeAZZLeEhHPH2CMlld1X5kfI+tBNhgdwP+qKz8plT9H9u36xcASoJfsDKb6s6ouoPFZVYcAN5Alpk1kZy8NNsYxI1c2huyD/7dpW+/JLTc7LVM7q2or2dHKVuAduXW8lOyIJ4Cr99MOHwN+SXbG1GfJBuUDuCDNr529NZCWWUNu3KBuXV1kH47PpPb9H2ldG4bY/kPsO8Yxh+zspN+SJcSraTzGsTZXpz+/nv383V4CfCf9zQL4cqO/B9nA+jKyRPME2SD823PbvJa9x7Fm1/+NG+zv/ta5z741WMftaZlbc2WH5dry/Kr/v0bLQ6lxzTqCpG8D84D/GBX/PsKsXbmryjqCpDeQJYxTyAZe1wxdw8wG48Fx6xTzyH6/8DPg/eFDbbMD5q4qMzMrxEccZmZWSEeMcUyaNClmzJhRdRhmZm1l3bp1v4qIfX6o2hGJY8aMGfT09FQdhplZW5HU8AeY7qoyM7NCnDjMzKwQJw4zMyuk1MQhaZ6kXklbJF3cYP50Sasl3S9praRpqfwUSRtyj2ckzU/zrpX0s9y8+msWmZlZiUobHJc0BriS7IJy28guPLcyIjblFrscuD4irpN0KnAZ2UX01pAuYifp5WQXYbszV29RRPhm9WZmFSjzrKoTya6WuhVA0s1kV/PMJ44uXrgL2hqyO7jVO4PsEtZPN5hXqhXr+1i6qpft/QNMmTiBRXNnMX/O1IMdhpnZiFJmV9VUXrhMM2RHHfWfuveR3W0OYAFwuKQj65Y5k+wWl3mXpu6tK9JtTofdivV9LF6+kb7+AQLo6x9g8fKNrFjfV8bmzMzaRtWD4xcCJ0laT3bZ7j5euJkM6f7QJwCrcnUWk92/+vVkl9G+iAYknSOpR1LPjh07Cge2dFUvAzt371U2sHM3S1f1Fl6XmdloUmbi6OOFG9xAdlOdvb6uR8T2iFgYEXOAT6ay/M1w3g3cHrlbW0bEI5F5FriGrEtsHxFxVUR0R0T35MnF79C5vX+gULmZWacoM3HcC8yUdFy6h/OZwMr8ApImSarFsJjsRi55Z1HXTZWOQpAksjupPVBC7EyZOKFQuZlZpygtcUTELuA8sm6mzcAtEfGgpCWS3pkWOxnolfQTsntI77lns6QZZEcs361b9Q2SNgIbye73/NdlxL9o7iwmjBuzV9mEcWNYNHfWIDXMzDpDR1xWvbu7Ow7kWlU+q8rMOpmkdRHRXV/eERc5PFDz50x1ojAzq1P1WVVmZtZmnDjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zMysECcOMzMrxInDzMwKceIwM7NCnDjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zMysECcOMzMrxInDzMwKceIwM7NCnDjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrJBSE4ekeZJ6JW2RdHGD+dMlrZZ0v6S1kqal8lMkbcg9npE0P807TtIP0zq/KWl8mftgZmZ7Ky1xSBoDXAmcBnQBZ0nqqlvscuD6iHgtsAS4DCAi1kTE7IiYDZwKPA3cmep8FrgiIl4JPA58uKx9MDOzfZV5xHEisCUitkbEc8DNwOl1y3QBd6fXaxrMBzgD+HZEPC1JZInk1jTvOmD+sEduZmaDKjNxTAUezk1vS2V59wEL0+sFwOGSjqxb5kzgpvT6SKA/InYNsU4AJJ0jqUdSz44dOw5wF8zMrF7Vg+MXAidJWg+cBPQBu2szJR0NnACsKrriiLgqIrojonvy5MnDFa+ZWccbW+K6+4BjctPTUtkeEbGddMQh6TDgXRHRn1vk3cDtEbEzTf8amChpbDrq2GedZmZWrjKPOO4FZqazoMaTdTmtzC8gaZKkWgyLgWV16ziLF7qpiIggGws5IxWdDdxRQuxmZjaI0hJHOiI4j6ybaTNwS0Q8KGmJpHemxU4GeiX9BDgKuLRWX9IMsiOW79at+iLg45K2kI15XF3WPpiZ2b6UfYkf3bq7u6Onp6fqMMzM2oqkdRHRXV9e9eC4mZm1GScOMzMrxInDzMwKceIwM7NCnDjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zMysECcOMzMrxInDzMwKceIwM7NCnDjMzKwQJw4zMyvEicPMzAoZW3UAo9mK9X0sXdXL9v4BpkycwKK5s5g/Z+pBq1+1do+/VZ2+/zZ6OXGUZMX6PhYv38jAzt0A9PUPsHj5RoCmPjxarV+1do+/VZ2+/za6uauqJEtX9e750KgZ2Lmbpat6D0r9qrV7/K3q9P230c2JoyTb+wcKlQ93/aq1e/yt6vT9t9HNiaMkUyZOKFQ+3PWr1u7xt6rT999GNyeOkiyaO4sJ48bsVTZh3BgWzZ11UOpXrd3jb1Wn77+Nbh4cL0ltAPRAz6pptX7V2j3+VnX6/tvopoioOobSdXd3R09PT9VhmJm1FUnrIqK7vtxdVWZmVogTh5mZFeLEYWZmhZSaOCTNk9QraYukixvMny5ptaT7Ja2VNC0371hJd0raLGmTpBmp/FpJP5O0IT1ml7kPZma2t9ISh6QxwJXAaUAXcJakrrrFLgeuj4jXAkuAy3LzrgeWRsRrgBOBx3LzFkXE7PTYUNY+mJnZvso84jgR2BIRWyPiOeBm4PS6ZbqAu9PrNbX5KcGMjYi7ACLiyYh4usRYzcysSWUmjqnAw7npbaks7z5gYXq9ADhc0pHAq4B+ScslrZe0NB3B1FyaureukHRIo41LOkdSj6SeHTt2DM8emZlZ5YPjFwInSVoPnAT0AbvJfpj45jT/9cDxwAdTncXAq1P5y4GLGq04Iq6KiO6I6J48eXKZ+2Bm1lGaShzpm//bJBVJNH3AMbnpaalsj4jYHhELI2IO8MlU1k92dLIhdXPtAlYAv5fmPxKZZ4FryLrEzMzsIGk2Efwt8F7gp5I+I6mZC+7cC8yUdJyk8cCZwMr8ApIm5ZLRYmBZru5ESbVDhVOBTanO0elZwHzggSb3wczMhkFTiSMi/iUi3kf2rf8h4F8k/ZukD0kaN0idXcB5wCpgM3BLRDwoaYmkd6bFTgZ6Jf0EOAq4NNXdTdZNtVrSRkDA11KdG1LZRmAS8NcF99nMzFrQ9LWq0qD1+4EPANuBG4A3ASdExMllBTgcfK0qM7PiBrtWVVNXx5V0OzAL+HvgHRHxSJr1TUn+RDYz6yDNXlb9ixGxptGMRtnIzMxGr2YHx7skTaxNSDpC0sdKisnMzEawZhPHR9JpsgBExOPAR8oJyczMRrJmE8eYdPorsOc6VOPLCcnMzEayZsc4vkM2EP7VNP3RVGZmZh2m2cRxEVmy+K9p+i7g66VEZGZmI1pTiSMinge+kh5mZtbBmv0dx0yye2V0AYfWyiPi+JLiMjOzEarZwfFryI42dgGnkN1k6RtlBWVmZiNXs4ljQkSsJrtEyc8j4hLgbeWFZWZmI1Wzg+PPpqvY/lTSeWSXRz+svLDMzGykavaI43zgxcB/A15HdrHDs8sKyszMRq79HnGkH/u9JyIuBJ4EPlR6VGZmNmLt94gj3RvjTQchFjMzawPNjnGsl7QS+AfgqVphRCwvJSozMxuxmk0chwK/JruFa00AThxmZh2m2V+Oe1zDzMyA5n85fg3ZEcZeIuJPhj0iMzMb0Zrtqvqn3OtDgQVk9x03M7MO02xX1W35aUk3AT8oJSIzMxvRmv0BYL2ZwO8MZyBmZtYemh3jeIK9xzh+SXaPDjMz6zDNdlUdXnYgZmbWHprqqpK0QNLLctMTJc0vLywzMxupmh3j+HRE/KY2ERH9wKfLCcnMzEayZhNHo+WaPZXXzMxGkWYTR4+kz0l6RXp8DlhXZmBmZjYyNZs4/hR4DvgmcDPwDHBuWUGZmdnI1exZVU8BF5cci5mZtYFmz6q6S9LE3PQRklY1UW+epF5JWyTtk3gkTZe0WtL9ktZKmpabd6ykOyVtlrRJ0oxUfpykH6Z1flPS+Gb2wczMhkezXVWT0plUAETE4+znl+PpzoFXAqcBXcBZkrrqFrscuD4iXgssAS7LzbseWBoRrwFOBB5L5Z8FroiIVwKPAx9uch/MzGwYNJs4npd0bG0iffvf52q5dU4EtkTE1oh4jmxs5PS6ZbqAu9PrNbX5KcGMjYi7ACLiyYh4WpLI7glya6pzHeDfk5iZHUTNJo5PAj+Q9PeSvgF8F1i8nzpTgYdz09tSWd59wML0egFwuKQjgVcB/ZKWS1ovaWk6gjkS6I+IXUOsEwBJ50jqkdSzY8eOJnfTzMz2p6nEERHfAbqBXuAm4BPAwDBs/0LgJEnrgZOAPmA32aD9m9P81wPHAx8ssuKIuCoiuiOie/LkycMQqpmZQfMXOfzPwPnANGAD8Abgf7P3rWTr9QHH5KanpbI9ImI76YhD0mHAuyKiX9I2YENEbE3zVqRtLgMmShqbjjr2WaeZmZWr2a6q88m++f88Ik4B5gD9Q1fhXmBmOgtqPHAmsDK/gKRJkmoxLCZLDLW6EyXVDhVOBTZFRJCNhZyRys8G7mhyH8zMbBg0mzieiYhnACQdEhE/BmYNVSEdEZwHrAI2A7dExIOSlkh6Z1rsZKBX0k+Ao4BLU93dZN1UqyVtBAR8LdW5CPi4pC1kYx5XN7kPZmY2DJR9id/PQtLtwIeAC8i+/T8OjIuIt5Yb3vDo7u6Onp6eqsMwM2srktZFRHd9ebO/HF+QXl4iaQ3wMuA7wxifmZm1icJXuI2I75YRiJmZtYcDvee4mZl1KCcOMzMrxInDzMwKceIwM7NCnDjMzKwQ3zd8FFuxvo+lq3rZ3j/AlIkTWDR3FvPnNLwmZCn1W9Xp2293Vbdfu7//RzInjlFqxfo+Fi/fyMDO3QD09Q+wePlGgKbe/K3Wb1Wnb7/dVd1+7f7+H+ncVTVKLV3Vu+dNXzOwczdLV/UelPqt6vTtt7uq26/d3/8jnRPHKLW9v/FV7wcrH+76rer07be7qtuv3d//I50Txyg1ZeKEQuXDXb9Vnb79dld1+7X7+3+kc+IYpRbNncWEcWP2KpswbgyL5g55UeNhq9+qTt9+u6u6/dr9/T/SeXB8lKoN4B3oWSGt1m9Vp2+/3VXdfu3+/h/pmrqservzZdXNzIob7LLq7qoyM7NCnDjMzKwQJw4zMyvEicPMzApx4jAzs0KcOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zMysECcOMzMrxInDzMwKceIwM7NCnDjMzKwQJw4zMyuk1MQhaZ6kXklbJF3cYP50Sasl3S9praRpuXm7JW1Ij5W58msl/Sw3b3aZ+2BmZnsr7Q6AksYAVwJ/CGwD7pW0MiI25Ra7HLg+Iq6TdCpwGfCBNG8gIgZLCosi4tayYjczs8GVecRxIrAlIrZGxHPAzcDpdct0AXen12sazDczsxGmzMQxFXg4N70tleXdByxMrxcAh0s6Mk0fKqlH0j2S5tfVuzR1b10h6ZBGG5d0Tqrfs2PHjhZ3xczMaqoeHL8QOEnSeuAkoA/YneZNT/e6fS/weUmvSOWLgVcDrwdeDlzUaMURcVVEdEdE9+TJk8vcBzOzjlJm4ugDjslNT0tle0TE9ohYGBFzgE+msv703JeetwJrgTlp+pHIPAtcQ9YlZmZmB0mZieNeYKak4ySNB84EVuYXkDRJUi2GxcCyVH5ErQtK0iTgjcCmNH10ehYwH3igxH0wM7M6pZ1VFRG7JJ0HrALGAMsi4kFJS4CeiFgJnAxcJimA7wHnpuqvAb4q6Xmy5PaZ3NlYN0iaDAjYAPyXsvbBzMz2pYioOobSdXd3R09PT9VhmJm1FUnr0ljzXqoeHDczszbjxGFmZoU4cZiZWSFOHGZmVogTh5mZFeLEYWZmhThxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4eZmRXixGFmZoU4cZiZWSFOHGZmVogTh5mZFeLEYWZmhThxmJlZIU4cZmZWiBOHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4eZmRXixGFmZoU4cZiZWSFOHGZmVogTh5mZFeLEYWZmhZSaOCTNk9QraYukixvMny5ptaT7Ja2VNC03b7ekDemxMld+nKQfpnV+U9L4MvfBzMz2VlrikDQGuBI4DegCzpLUVbfY5cD1EfFaYAlwWW7eQETMTo935so/C1wREa8EHgc+XNY+mJnZvso84jgR2BIRWyPiOeBm4PS6ZbqAu9PrNQ3m70WSgFOBW1PRdcD8YYvYzMz2q8zEMRV4ODe9LZXl3QcsTK8XAIdLOjJNHyqpR9I9kmrJ4UigPyJ2DbFOMzMr0diKt38h8GVJHwS+B/QBu9O86RHRJ+l44G5JG4HfNLtiSecA5wAce+yxwxq0tYcV6/tYuqqX7f0DTJk4gUVzZzF/Tvt8z2g1/qrrt6rq7bei6rYvu+3KTBx9wDG56WmpbI+I2E464pB0GPCuiOhP8/rS81ZJa4E5wG3ARElj01HHPuvMrfsq4CqA7u7uGL7dsnawYn0fi5dvZGBn9j2kr3+Axcs3ArTFh0+r8Vddv1VVb78VVbf9wWi7Mruq7gVmprOgxgNnAivzC0iaJKkWw2JgWSo/QtIhtWWANwKbIiLIxkLOSHXOBu4ocR+sTS1d1bvnH6dmYOdulq7qrSiiYlqNv+r6rap6+62ouu0PRtuVljjSEcF5wCpgM3BLRDwoaYmk2llSJwO9kn4CHAVcmspfA/RIuo8sUXwmIjaleRcBH5e0hWzM4+qy9sHa1/b+gULlI02r8Vddv1VVb78VVbf9wWi7Usc4IuJbwLfqyj6Ve30rL5whlV/m34ATBlnnVrIztswGNWXiBPoa/KNMmTihgmiKazX+quu3qurtt6Lqtj8YbedfjtuotGjuLCaMG7NX2YRxY1g0d1ZFERXTavxV129V1dtvRdVtfzDaruqzqsxKURsEbNezclqNv+r6rap6+62ouu0PRtspG28e3bq7u6Onp6fqMMzM2oqkdRHRXV/uriozMyvEicPMzApx4jAzs0KcOMzMrBAnDjMzK6QjzqqS9AQw8q9VMHJNAn5VdRBtzO3XGrffgWu17aZHxOT6wk75HUdvo1PKrDmSetx+B87t1xq334Erq+3cVWVmZoU4cZiZWSGdkjiuqjqANuf2a43brzVuvwNXStt1xOC4mZkNn0454jAzs2HixGFmZoWM6sQhaZ6kXklbJF1cdTztRtJDkjZK2iDJlxfeD0nLJD0m6YFc2csl3SXpp+n5iCpjHMkGab9LJPWl9+AGSW+tMsaRTNIxktZI2iTpQUnnp/Jhfw+O2sQhaQxwJXAa0AWcJamr2qja0ikRMdvn0TflWmBeXdnFwOqImAmsTtPW2LXs234AV6T34Ox0V1FrbBfwiYjoAt4AnJs+84b9PThqEwfZ7WW3RMTWiHgOuBk4veKYbBSLiO8B/6+u+HTguvT6OmD+QQ2qjQzSftakiHgkIn6UXj8BbAamUsJ7cDQnjqnAw7npbanMmhfAnZLWSTqn6mDa1FER8Uh6/UvgqCqDaVPnSbo/dWW5q68JkmYAc4AfUsJ7cDQnDmvdmyLi98i6+86V9JaqA2pnkZ377vPfi/kK8ApgNvAI8DfVhjPySToMuA24ICJ+m583XO/B0Zw4+oBjctPTUpk1KSL60vNjwO1k3X9WzKOSjgZIz49VHE9biYhHI2J3RDwPfA2/B4ckaRxZ0rghIpan4mF/D47mxHEvMFPScZLGA2cCKyuOqW1Ieomkw2uvgf8EPDB0LWtgJXB2en02cEeFsbSd2gdesgC/BwclScDVwOaI+Fxu1rC/B0f1L8fTqXufB8YAyyLi0opDahuSjic7yoDsKso3uv2GJukm4GSyS1k/CnwaWAHcAhwL/Bx4d0R4ALiBQdrvZLJuqgAeAj6a66+3HElvAr4PbASeT8V/TjbOMazvwVGdOMzMbPiN5q4qMzMrgROHmZkV4sRhZmaFOHGYmVkhThxmZlaIE4dZBSTNyF8F1qydOHGYmVkhThxmFZN0vKT1kl5fdSxmzRhbdQBmnUzSLLJL/n8wIu6rOh6zZjhxmFVnMtl1gxZGxKaqgzFrlruqzKrzG+AXwJuqDsSsCB9xmFXnObIrvq6S9GRE3Fh1QGbNcOIwq1BEPCXp7cBdKXn40v824vnquGZmVojHOMzMrBAnDjMzK8SJw8zMCnHiMDOzQpw4zMysECcOMzMrxInDzMwK+f9J20v1jk4C/AAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}